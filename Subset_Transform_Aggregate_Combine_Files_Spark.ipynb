{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Top\"></a>\n",
    "# Using Spark to Prepare a Large, Detailed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Table of Contents\n",
    "- [Introduction](#Intro)\n",
    "- [Import necessary libraries and packages](#Imports)\n",
    "- [Read in a previously persisted ID list](#IDlist)\n",
    "- [Define a function to prepare monthly aggregated data for each table](#AggFile)\n",
    "- [Table schemas and required transformations and aggregations](#Schemas)\n",
    "- [Run subset and aggregation for all months for all tables](#Process)\n",
    "- [Merge all tables](#Merge)\n",
    "- [Aggregate remaining constant/current columns into one](#Constants)\n",
    "- [Write the results to file](#Output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Intro\"></a>\n",
    "## Introduction\n",
    "In data science, more data is usually better. But because a lot of the activity of a data scientist is exploratory and iterative, a large data volume comes at a cost. When it takes hours just to read the data into memory and perform preliminary transformations, it is hard to make progress. So it can be useful to start with a subset of the data and aggregate too much detail into a coarser-grained representation. You can always go back to the original volume and level of detail, once the feature engineering and modeling approach have been developed and validated.\n",
    "\n",
    "This notebook illustrates the following operations on a large dataset consisting of multiple files representing detailed records over several data collection periods (for example, daily detail for months or quarters) for several distinct source tables (for example, customers, usage or transactions, billing, etc.).\n",
    "\n",
    "1. Create a manageable subset of the full dataset by selecting specific unique IDs and aggregating detail data.<br>\n",
    "   See separate notebook *Generate_Subset_By_Column_Spark* for the generation of the list of IDs and a further discussion of the need for subsampling.\n",
    "\n",
    "1. Aggregate detail data.<br>\n",
    "   For example, aggregate daily detail into months.\n",
    "\n",
    "1. Apply data transformations as needed.<br>\n",
    "   This includes dropping columns, computing derived columns, changing data types, normalizing and scaling, and more.\n",
    "\n",
    "1. Merge all files into a single DataFrame.<br>\n",
    "   Using union or join operations depending on the type of attributes involved, roll all data from all files into a single DataFrame. The common key over which the different tables can be joined or aggregated is the unique ID.\n",
    "\n",
    "In the example below, the period is a month, and each file contains daily detail, which needs to be aggregated by month. The example is based on a churn use case, which has its own peculiar characteristics—mainly, that the entities being represented (users/customers) join and leave over the course of the data collection period.\n",
    "\n",
    "Most of the monthly values are kept separate; for a given attribute, the final dataset will have as many columns for that attribute as there are months in the input dataset. Other attributes, those that are either not expected to change or for which only the current value matters, are collapsed into a single column.\n",
    "\n",
    "Note that the example is based on a dataset that is too small to require any of this code; it is just enough to let the code run and show its intended operation.\n",
    "\n",
    "[Top **⤒**](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- The tables below are easier to keep with the surrounding text if they are left-justified instead of centered -->\n",
       "<style>\n",
       "table {float:left}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<!-- The tables below are easier to keep with the surrounding text if they are left-justified instead of centered -->\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### For example, if the two tables for the first two months look like this:\n",
    "30 (or 31 or 28) rows for each UID. (UID = Unique IDentifier; actual column name may vary, of course.) Not all UIDs may appear in all months, as customers join and leave.\n",
    "\n",
    "Table1, January|Table1, February|Table2, January|Table2, February\n",
    ":---:|:---:|:---:|:---:\n",
    "<table> <thead> <tr> <th>UID</th> <th>Day</th> <th>Col1</th> <th>Col2</th> <th>...</th> </tr> </thead> <tbody> <tr> <td>001</td> <td style=\"text-align:right;\">1</td> <td><strong>ABC</strong></td> <td>XYZ</td> <td>...</td> </tr> <tr> <td>001</td> <td style=\"text-align:right;\">2</td> <td><strong>ABC</strong></td> <td>XYZ</td> <td>...</td> </tr> <tr> <td>001</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> <tr> <td>001</td> <td style=\"text-align:right;\">31</td> <td><strong><strong>ABC</strong></strong></td> <td>XYZ</td> <td>...</td> </tr> <tr> <td>002</td> <td style=\"text-align:right;\">1</td> <td>DEF</td> <td>PQR</td> <td>...</td> </tr> <tr> <td>002</td> <td style=\"text-align:right;\">2</td> <td>DEF</td> <td>PQR</td> <td>...</td> </tr> <tr> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> </tbody> </table>|<table> <thead> <tr> <th>UID</th> <th>Day</th> <th>Col1</th> <th>Col2</th> <th>...</th> </tr> </thead> <tbody> <tr> <td>001</td> <td style=\"text-align:right;\">1</td> <td><strong>ABC</strong></td> <td>XYZ</td> <td>...</td> </tr> <tr> <td>001</td> <td style=\"text-align:right;\">2</td> <td><strong>DCB</strong></td> <td>XYZ</td> <td>...</td> </tr> <tr> <td>001</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> <tr> <td>001</td> <td style=\"text-align:right;\">28</td> <td><strong>DCB</strong></td> <td>XYZ</td> <td>...</td> </tr> <tr> <td>002</td> <td style=\"text-align:right;\">1</td> <td>DEF</td> <td>PQR</td> <td>...</td> </tr> <tr> <td>002</td> <td style=\"text-align:right;\">2</td> <td>DEF</td> <td>PQR</td> <td>...</td> </tr> <tr> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> </tbody> </table>|<table> <thead> <tr> <th>UID</th> <th>Day</th> <th>ColA</th> <th>ColB</th> <th>...</th> </tr> </thead> <tbody> <tr> <td>001</td> <td style=\"text-align:right;\">1</td> <td style=\"text-align:right;\">123</td> <td style=\"text-align:right;\">1.3</td> <td>...</td> </tr> <tr> <td>001</td> <td style=\"text-align:right;\">2</td> <td style=\"text-align:right;\">234</td> <td style=\"text-align:right;\">2.4</td> <td>...</td> </tr> <tr> <td>001</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> <tr> <td>001</td> <td style=\"text-align:right;\">31</td> <td style=\"text-align:right;\">213</td> <td style=\"text-align:right;\">1.7</td> <td>...</td> </tr> <tr> <td>002</td> <td style=\"text-align:right;\">1</td> <td style=\"text-align:right;\">567</td> <td style=\"text-align:right;\">5.7</td> <td>...</td> </tr> <tr> <td>002</td> <td style=\"text-align:right;\">2</td> <td style=\"text-align:right;\">678</td> <td style=\"text-align:right;\">6.8</td> <td>...</td> </tr> <tr> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> </tbody> </table>|<table> <thead> <tr> <th>UID</th> <th>Day</th> <th>ColA</th> <th>ColB</th> <th>...</th> </tr> </thead> <tbody> <tr> <td>001</td> <td style=\"text-align:right;\">1</td> <td style=\"text-align:right;\">345</td> <td style=\"text-align:right;\">1.5</td> <td>...</td> </tr> <tr> <td>001</td> <td style=\"text-align:right;\">2</td> <td style=\"text-align:right;\">456</td> <td style=\"text-align:right;\">2.6</td> <td>...</td> </tr> <tr> <td>001</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> <tr> <td>001</td> <td style=\"text-align:right;\">28</td> <td style=\"text-align:right;\">333</td> <td style=\"text-align:right;\">5.1</td> <td>...</td> </tr> <tr> <td>002</td> <td style=\"text-align:right;\">1</td> <td style=\"text-align:right;\">678</td> <td style=\"text-align:right;\">5.9</td> <td>...</td> </tr> <tr> <td>002</td> <td style=\"text-align:right;\">2</td> <td style=\"text-align:right;\">789</td> <td style=\"text-align:right;\">6.2</td> <td>...</td> </tr> <tr> <td>...</td> <td>...</td> <td>...</td> <td>...</td> <td>...</td> </tr> </tbody> </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Then the desired result, after aggregating and combining, looks like this:\n",
    "\n",
    "The aggregation function (First, Last, Max, or Sum) is indicated for each column.\n",
    "\n",
    "Notice that in the example, the **Col1** value for UID=001 changes in February, and that is\n",
    "reflected in the aggregate for **Col1** in the table below, which is based on the Last occurrence.\n",
    "\n",
    "In other words, some attributes (the ones representing state and aggregated with First and Last\n",
    "functions) are aggregated over the entire data collection period; others (the ones representing\n",
    "counts or measures and aggregated with Sum or Max functions) are aggregated by month, with the\n",
    "monthly aggregates kept in separate columns.\n",
    "\n",
    "Applying this, the table will look as follows.\n",
    "\n",
    "UID|Col1<br>(Last)|Col2<br>(First)|...|ColA_Jan<br>(Max)|ColB_JAN<br>(Sum)|...|ColA_FEB<br>(Max)|ColB_FEB<br>(Sum)|...\n",
    "---|:---:|:---:|---|---:|---:|---|---:|---:|---|---:|---:|---\n",
    "001|**DCB**|XYZ|...|963|45.3|...|852|57.1|...\n",
    "002|DEF|PQR|...|974|127.2|...|865|108.9|...\n",
    "...|...|...|...|...|...|...|...|...|...\n",
    "\n",
    "[Top **⤒**](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Imports\"></a>\n",
    "## Import necessary libraries and packages\n",
    "And set up the Spark session.\n",
    "\n",
    "[Top **⤒**](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "from pyspark.sql           import SparkSession\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import array\n",
    "\n",
    "spark = SparkSession(sc).builder.getOrCreate()\n",
    "\n",
    "# Path to project datasets\n",
    "import os\n",
    "local_path = os.environ['DSX_PROJECT_DIR'] + '/datasets/'\n",
    "\n",
    "# Handy package for interacting with iterables\n",
    "import itertools\n",
    "\n",
    "# Used for the monthly data in this example: naming files and columns\n",
    "from calendar import month_abbr\n",
    "\n",
    "# Handy packages for reporting running time and wall clock time\n",
    "from time     import time\n",
    "from datetime import datetime, timedelta\n",
    "from pytz     import timezone\n",
    "\n",
    "# Bring in a custom function to write CSV from a Spark DataFrame as a single file\n",
    "import sys\n",
    "sys.path.insert(0, '../scripts')\n",
    "from spark1csv import spark_write_one_csv\n",
    "\n",
    "# Use logging to see output from the custom function\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"IDlist\"></a>\n",
    "## Read in a previously persisted ID list\n",
    "In this example, assume that a useful subset (2 columns: ID + label) was persisted in a Parquet file.\n",
    "\n",
    "[Top **⤒**](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### First set the specifics: path, filename, and relevant column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dir_path      = '/user-home/libraries/UserData/datasets/'    # Set the directory path, if in a library\n",
    "dir_path      = local_path                                   # Set the directory path, if in this project\n",
    "\n",
    "# Apply your own naming convention for the ID list input and the result output files\n",
    "suffix        = '_0k_35'\n",
    "id_file       = 'customers' + suffix + '.parquet' # Set this to the name of the file containing the subset specification\n",
    "out_file      = 'subset_merge_agg' + suffix\n",
    "\n",
    "# \n",
    "id_column     = 'customer_id'\n",
    "label_column  = 'churned'\n",
    "\n",
    "suffixes      = [month_abbr[i] for i in range(1,13)]     # For month-based file and column names\n",
    "file_type     = '.csv'                                   # '.csv' or '.csv.gz' or whatever the filename extension is\n",
    "\n",
    "# For convenience in reporting progress, \n",
    "# set this according to local timezone of browser (not the Jupyter client or DSX server or UTC)\n",
    "tz = timezone('US/Pacific')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read the list of unique IDs and their associated labels\n",
    "The resulting DataFrame is used to subset the whole dataset (using joins)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- churned: integer (nullable = true)\n",
      " |-- period_count: long (nullable = true)\n",
      "\n",
      "Number of IDs: 694\n"
     ]
    }
   ],
   "source": [
    "# Cache the DataFrame to avoid repeated reading (not sure if this matters)\n",
    "df_subset_ids = spark.read.parquet(dir_path + id_file).cache()\n",
    "\n",
    "df_subset_ids.printSchema()\n",
    "\n",
    "# Counting the rows forces Spark to perform the read and instantiate the DataFrame\n",
    "n_ids = df_subset_ids.count()\n",
    "print('Number of IDs: {}'.format(n_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Make sure there are no obvious problems that would derail everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The following statements raise an exception if something is amiss, stopping execution before it can generate useless results\n",
    "\n",
    "# 1. Make sure the ID list is not empty\n",
    "assert n_ids > 0, 'List of IDs is empty.'\n",
    "\n",
    "# 2. Make sure there are no duplicates\n",
    "assert df_subset_ids.agg(countDistinct(id_column)).first()[0] == n_ids, 'List of IDs contains duplicates.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"AggFile\"></a>\n",
    "## Define a function to prepare monthly aggregated data for each table\n",
    "Interpret \"month\" loosely; your data may be organized by some other time interval or variable.\n",
    "\n",
    "[Top **⤒**](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def agg_by_file(df_ids, table, suffix, ftype, fpath, schema=None, header=True, transforms=None, dropcols=[], agg_query=None):\n",
    "    '''\n",
    "    Aggregate daily data into monthly aggregates.\n",
    "    \n",
    "    This function only applies DataFrame transformations, no actions. Therefore, it will\n",
    "    seem very fast, but the real work will not happen until an action later on forces the\n",
    "    execution of the processing graph.\n",
    "    \n",
    "    Parameters:\n",
    "        df_ids     DataFrame with single column, containing unique IDs\n",
    "        table      Table name: for example, Users or Usage\n",
    "        suffix     Suffix used for file names and column names (in the example, 3-character months: Jan, Feb, etc.)\n",
    "        ftype      Filename extension (for example, '.csv')\n",
    "        fpath      Path to input file directory\n",
    "        schema     Full pyspark DataFrame schema of input data\n",
    "        header     Whether the files contain a header line\n",
    "        transforms Function that applies custom DataFrame transformations, if necessary. May be None.\n",
    "                   Must take DataFrame as a parameter.\n",
    "        dropcols   List of column names to drop. May be empty.\n",
    "        agg_query  SQL query string to perform the table- and column-specific aggregations. May be None\n",
    "                   Must have numbered string format placeholders ('{0}') for the suffix in each column alias.\n",
    "    \n",
    "    Assumptions:\n",
    "        df_ids is not empty and contains no duplicates.\n",
    "        File naming convention is fixed.\n",
    "        Path and file exist\n",
    "        Dropping columns before applying transformations is fine; any columns that need to exist for\n",
    "        the transformations and dropped later should be dropped as part of the transforms function\n",
    "    \n",
    "    Limitations:\n",
    "        Limited parameter error checking\n",
    "    '''\n",
    "    \n",
    "    # Construct filename according to convention. Here: Users_MMM.csv.gz or Usage_MMM.csv.gz (MMM is JAN, FEb, ...)\n",
    "    filename = os.path.join(fpath, table + '_' + suffix + ftype)\n",
    "    \n",
    "    df_extract = (spark.read.csv(filename, schema=schema)\n",
    "                  .drop(*dropcols)         # Immediately drop the non-required columns\n",
    "                  .join(df_ids,            # and subset the data\n",
    "                        on=[id_column],    # Special syntax if join column has the same name on both sides\n",
    "                        how='inner')       # Making sure every CO_ID gets a row happens later, when all tables are joined\n",
    "                 )\n",
    "    \n",
    "    if transforms:\n",
    "        df_extract = transforms(df_extract)\n",
    "    \n",
    "    if agg_query:\n",
    "        # A SQL query performs the aggregation over the month.\n",
    "        # First replace the placeholders ('{0}') in the projection part (SELECT clause) of the provided query with the suffix\n",
    "        query = agg_query.format(suffix)\n",
    "        df_extract.createOrReplaceTempView('extract')\n",
    "        df_extract = spark.sql(query)\n",
    "    \n",
    "    return df_extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Schemas\"></a>\n",
    "## Table schemas and required transformations and aggregations\n",
    "In this example, assume that we know what the schemas are from prior work. It is better to apply those, rather than letting Spark\n",
    "infer the schema, even if the input files contain headers. For one, CSV headers only contain names, not data types. For another,\n",
    "inferring the schema requires an additional pass over the data, dramatically increasing the time it takes to read the files.\n",
    "\n",
    "Required transformations (columns to be dropped or transformed)\n",
    "and aggregations are decided by detailed examination, outside this notebook.\n",
    "\n",
    "A major purpose of this notebook is to aggregate detail (e.g., daily) data into summary (e.g., monthly) data. How to do that\n",
    "depends on the nature of each columnn. Support for SQL in Spark lets us express these in relatively readable syntax, using\n",
    "standard aggregate functions:\n",
    "- For static columns (the value does not change over the entire period, such as _Gender_), pick any occurrence: `First()`\n",
    "- For columns indicating a status, which may change along the way: `Last()`\n",
    "- For numeric columns, pick an appropriate roll-up: `Max()` or `Sum()`\n",
    "\n",
    "**Note** the use of format placeholders (`{0}`) in the SQL query strings. These are used to make the query specific to\n",
    "each file (representing months, in this example) and name the resultant columns accordingly.\n",
    "\n",
    "[Top **⤒**](#Top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Example of \"prior work\" to determine file schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField(customer_id,StringType,true),\n",
       " StructField(first_name,StringType,true),\n",
       " StructField(last_name,StringType,true),\n",
       " StructField(twitter_handle,StringType,true),\n",
       " StructField(number,StringType,true),\n",
       " StructField(gender,StringType,true),\n",
       " StructField(age,StringType,true),\n",
       " StructField(type,StringType,true),\n",
       " StructField(location,StringType,true),\n",
       " StructField(location_lat,StringType,true),\n",
       " StructField(location_lon,StringType,true),\n",
       " StructField(callcenter_callcount,StringType,true),\n",
       " StructField(text_package_deal,StringType,true),\n",
       " StructField(voice_package_deal,StringType,true),\n",
       " StructField(4g_handset_deal,StringType,true),\n",
       " StructField(all_u_can_eat_data_deal,StringType,true),\n",
       " StructField(accidental_damage_cover_deal,StringType,true),\n",
       " StructField(europe_roamer_deal,StringType,true),\n",
       " StructField(americas_roamer_deal,StringType,true),\n",
       " StructField(asia_africa_roamer_deal,StringType,true),\n",
       " StructField(extra_handset_deal,StringType,true),\n",
       " StructField(musicsubscription_deal,StringType,true),\n",
       " StructField(churned,StringType,true)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just read one file containing headers and see what Spark infers. Do this for each table type.\n",
    "spark.read.csv(os.path.join(local_path, 'customers_Jan.csv'), header=True).schema.fields\n",
    "\n",
    "# spark.read.csv(os.path.join(local_path, 'calls_Jan.csv'), header=True).schema.fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Users\n",
    "Based on schema shown above. In the sample data, users are called customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This schema only applies to the sample data\n",
    "users_schema = StructType([\n",
    "    StructField('customer_id'                 , StringType(), False),    # The ID column should not have nulls\n",
    "    StructField('first_name'                  , StringType(), True ),\n",
    "    StructField('last_name'                   , StringType(), True ),\n",
    "    StructField('twitter_handle'              , StringType(), True ),\n",
    "    StructField('number'                      , StringType(), True ),\n",
    "    StructField('gender'                      , StringType(), True ),\n",
    "    StructField('age'                         , StringType(), True ),\n",
    "    StructField('type'                        , StringType(), True ),\n",
    "    StructField('location'                    , StringType(), True ),\n",
    "    StructField('location_lat'                , StringType(), True ),\n",
    "    StructField('location_lon'                , StringType(), True ),\n",
    "    StructField('callcenter_callcount'        , StringType(), True ),\n",
    "    StructField('text_package_deal'           , StringType(), True ),\n",
    "    StructField('voice_package_deal'          , StringType(), True ),\n",
    "    StructField('4g_handset_deal'             , StringType(), True ),\n",
    "    StructField('all_u_can_eat_data_deal'     , StringType(), True ),\n",
    "    StructField('accidental_damage_cover_deal', StringType(), True ),\n",
    "    StructField('europe_roamer_deal'          , StringType(), True ),\n",
    "    StructField('americas_roamer_deal'        , StringType(), True ),\n",
    "    StructField('asia_africa_roamer_deal'     , StringType(), True ),\n",
    "    StructField('extra_handset_deal'          , StringType(), True ),\n",
    "    StructField('musicsubscription_deal'      , StringType(), True ),\n",
    "    StructField('churned'                     , StringType(), False)     # The label column should not have nulls\n",
    "])\n",
    "\n",
    "# Drop columns considered unimportant or unreliable.\n",
    "# This just makes explicit which columns will be ignored. Strictly speaking there's\n",
    "# no need to drop them separately; they'll be dropped anyway by not appearing in the\n",
    "# SQL query below.\n",
    "\n",
    "# This set only applies to the sample data.\n",
    "users_dropcols = [\n",
    "    'first_name'    ,   # Personal, not relevant\n",
    "    'last_name'     ,   # Personal, not relevant\n",
    "    'twitter_handle',   # Personal, not relevant, often missing\n",
    "    'number'        ,   # One-to-one correlated with customer_id (NOTE: this may not be true in real use cases)\n",
    "    'location_lat'  ,   # No geospatial analysis in this use case\n",
    "    'location_lon'  ,   # No geospatial analysis in this use case\n",
    "    'churned'           # Use the label column from the selected ID list (df_subset_ids) instead\n",
    "]\n",
    "\n",
    "# Set up mapping functions and associated UDFs, as needed\n",
    "# This one only applies to the sample data\n",
    "\n",
    "# Reduce to broader categories: from location string ('AB12' ,etc.) take only the first letter.\n",
    "# This brings the number of unique values to 23, down from almost one for each row.\n",
    "map_location = udf(lambda loc: loc[:1])   \n",
    "\n",
    "# Transform some of the columns, using UDFs defined above as necessary\n",
    "def users_transforms(df_in):\n",
    "    '''\n",
    "    Apply required transformations that are specific to the Users table.\n",
    "    '''\n",
    "    df = df_in.withColumn('location' ,map_location('location'))\n",
    "    \n",
    "    # Several binary columns use 'Y' and 'N'. Convert them to using 1 and 0 instead,\n",
    "    # for ease of aggregation and modeling.\n",
    "    yes_no_cols = [c for c in df.columns if c.endswith('deal')]\n",
    "    for c in yes_no_cols:\n",
    "        df = df.withColumn(c, col(c).cast('boolean').cast('integer'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# For aggregation, apply a SQL query using the aggregate functions First, Last, and Max.\n",
    "# This is only needed if the detail files need to be aggregated, for example, if they\n",
    "# contain daily records and you need them by month. User data tends to change infrequently\n",
    "# and in most use cases will not be provided as daily detail.\n",
    "\n",
    "# The sample \"customers\" data is already by month; no aggregation of daily detail is needed.\n",
    "# But the files will be merged by unioning, meaning that the columns remain what they are but\n",
    "# new rows are added for each month, so aggregation is still needed to end up with one row\n",
    "# per user. In principle it can just be done once, after all months are merged, instead of\n",
    "# within each month.\n",
    "# However, if a static table like this one (none of the values vary day to day or even month\n",
    "# to month) does contain (highly redundant) daily detail, applying the aggregates to each\n",
    "# month anyway and not waiting till the final aggregation at the end may serve to keep memory\n",
    "# consumption down. More important, it lets us attach a month indicator (\"period\") to each\n",
    "# row, which will be used later to sort the rows before applying the Last() aggregate.\n",
    "# In this example, we'll apply the aggregate query both to each month (a no-op, since each\n",
    "# customer_id group has only one row) and after merging.\n",
    "users_query = '''\n",
    "SELECT customer_id\n",
    "     , First('gender'                      ) AS gender\n",
    "     , Last ('age'                         ) AS age\n",
    "     , Last ('type'                        ) AS type\n",
    "     , Last ('location'                    ) AS location\n",
    "     , Last ('callcenter_callcount'        ) AS callcenter_callcount\n",
    "     , Last ('text_package_deal'           ) AS text_package_deal\n",
    "     , Last ('voice_package_deal'          ) AS voice_package_deal\n",
    "     , Last ('4g_handset_deal'             ) AS 4g_handset_deal\n",
    "     , Last ('all_u_can_eat_data_deal'     ) AS all_u_can_eat_data_deal\n",
    "     , Last ('accidental_damage_cover_deal') AS accidental_damage_cover_deal\n",
    "     , Last ('europe_roamer_deal'          ) AS europe_roamer_deal\n",
    "     , Last ('americas_roamer_deal'        ) AS americas_roamer_deal\n",
    "     , Last ('asia_africa_roamer_deal'     ) AS asia_africa_roamer_deal\n",
    "     , Last ('extra_handset_deal'          ) AS extra_handset_deal\n",
    "     , Last ('musicsubscription_deal'      ) AS musicsubscription_deal\n",
    "     , '{0}'                                 AS period\n",
    "  FROM extract\n",
    " GROUP BY customer_id\n",
    "'''\n",
    "\n",
    "# Dictionary of function call arguments\n",
    "users_args  = {'fpath'       : dir_path        ,\n",
    "               'schema'      : users_schema    ,\n",
    "               'transforms'  : users_transforms,\n",
    "               'dropcols'    : users_dropcols  ,\n",
    "               'agg_query'   : users_query\n",
    "              }\n",
    "\n",
    "# Merge type: 'union' if all attributes are essentially constant and to be\n",
    "# aggregated with First() or Last(); otherwise, 'join'.\n",
    "users_merge = 'union'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Usage\n",
    "Based on schema shown above. In the sample data, usage is represented by calls only. In more realistic datasets for the telco industry, usage would include other things, such as data (MB) and texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This schema only applies to the sample data\n",
    "# NOTE: only impose non-string data types if you are sure there are no \n",
    "# funny values in the files or if you can afford to ignore records\n",
    "# that cause parse failures\n",
    "usage_schema = StructType([\n",
    "    StructField('customer_id'  , StringType (), False),\n",
    "    StructField('number'       , StringType (), True ),\n",
    "    StructField('date'         , DateType   (), True ),\n",
    "    StructField('from_calls'   , IntegerType(), True ),\n",
    "    StructField('from_duration', IntegerType(), True ),\n",
    "    StructField('from_dropped' , FloatType  (), True ),\n",
    "    StructField('to_calls'     , IntegerType(), True ),\n",
    "    StructField('to_duration'  , IntegerType(), True ),\n",
    "    StructField('to_dropped'   , FloatType  (), True )\n",
    "])\n",
    "\n",
    "# Use all but one column\n",
    "usage_dropcols = ['number']  # One-to-one correlated with customer_id (NOTE: this may not be true in real use cases)\n",
    "\n",
    "# Most of the column types are already coerced by providing the schema\n",
    "# to the CSV reader. Otherwise you'd cast them here. The only oddity to be corrected\n",
    "# in the sample data is that the dropped flags come as values with decimal fractions\n",
    "# (which are always zero).\n",
    "def usage_transforms(df_in):\n",
    "    return (df_in\n",
    "            .dropna()      # Placehoder for protecting numeric type casts; sample data has no missing or strange values\n",
    "#             .withColumn('date'         , col('date'         ).cast('date'   ))\n",
    "#             .withColumn('from_calls'   , col('from_calls'   ).cast('integer'))\n",
    "#             .withColumn('from_duration', col('from_duration').cast('integer'))\n",
    "            .withColumn('from_dropped' , col('from_dropped' ).cast('integer'))\n",
    "#             .withColumn('to_calls'     , col('to_calls'     ).cast('integer'))\n",
    "#             .withColumn('to_duration'  , col('to_duration'  ).cast('integer'))\n",
    "            .withColumn('to_dropped'   , col('to_dropped'   ).cast('integer'))\n",
    "           )\n",
    "\n",
    "# Note the aggregate function Sum, applied to all numeric columns\n",
    "# This query only applies to the sample data\n",
    "usage_query = '''\n",
    "SELECT customer_id                            ,\n",
    "       Sum(from_calls)    AS from_calls_{0}   , \n",
    "       Sum(from_duration) AS from_duration_{0},\n",
    "       Sum(from_dropped)  AS from_dropped_{0} ,\n",
    "       Sum(to_calls)      AS to_calls_{0}     ,\n",
    "       Sum(to_duration)   AS to_duration_{0}  ,\n",
    "       Sum(to_dropped)    AS to_dropped_{0}\n",
    "  FROM extract\n",
    " GROUP BY customer_id\n",
    "'''\n",
    "\n",
    "# Dictionary of function call arguments\n",
    "usage_args  = {'fpath'       : dir_path        ,\n",
    "               'schema'      : usage_schema    ,\n",
    "               'transforms'  : usage_transforms,\n",
    "               'dropcols'    : usage_dropcols  ,\n",
    "               'agg_query'   : usage_query\n",
    "              }\n",
    "\n",
    "# Merge type: 'union' if all attributes are essentially constant and to be\n",
    "# aggregated with First() or Last(); otherwise, 'join'.\n",
    "usage_merge = 'join'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Process\"></a>\n",
    "## Run subset and aggregation for all months for all tables\n",
    "Initially, keep all tables and months in separate DataFrames. Combine them later.\n",
    "\n",
    "[Top **⤒**](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set up the call arguments and merge methods for each of the tables as elements of a dictionary\n",
    "call_args = {\n",
    "    'customers' : users_args,\n",
    "    'calls'     : usage_args\n",
    "}\n",
    "\n",
    "merge_method = {\n",
    "    'customers' : users_merge,\n",
    "    'calls'     : usage_merge\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started aggregation at 21:29:02 03-21-2019\n",
      "Processing table     calls_Jan ... 499 rows processed in 0:00:04.\n",
      "Processing table     calls_Feb ... 510 rows processed in 0:00:02.\n",
      "Processing table     calls_Mar ... 531 rows processed in 0:00:01.\n",
      "Processing table     calls_Apr ... 528 rows processed in 0:00:01.\n",
      "Processing table     calls_May ... 532 rows processed in 0:00:01.\n",
      "Processing table     calls_Jun ... 539 rows processed in 0:00:01.\n",
      "Processing table     calls_Jul ... 524 rows processed in 0:00:01.\n",
      "Processing table     calls_Aug ... 513 rows processed in 0:00:01.\n",
      "Processing table     calls_Sep ... 508 rows processed in 0:00:01.\n",
      "Processing table     calls_Oct ... 492 rows processed in 0:00:01.\n",
      "Processing table     calls_Nov ... 493 rows processed in 0:00:01.\n",
      "Processing table     calls_Dec ... 468 rows processed in 0:00:01.\n",
      "Processing table customers_Jan ... 510 rows processed in 0:00:02.\n",
      "Processing table customers_Feb ... 531 rows processed in 0:00:02.\n",
      "Processing table customers_Mar ... 551 rows processed in 0:00:02.\n",
      "Processing table customers_Apr ... 546 rows processed in 0:00:02.\n",
      "Processing table customers_May ... 559 rows processed in 0:00:02.\n",
      "Processing table customers_Jun ... 561 rows processed in 0:00:01.\n",
      "Processing table customers_Jul ... 545 rows processed in 0:00:02.\n",
      "Processing table customers_Aug ... 530 rows processed in 0:00:01.\n",
      "Processing table customers_Sep ... 526 rows processed in 0:00:01.\n",
      "Processing table customers_Oct ... 512 rows processed in 0:00:01.\n",
      "Processing table customers_Nov ... 493 rows processed in 0:00:01.\n",
      "Processing table customers_Dec ... 468 rows processed in 0:00:02.\n",
      "Finished aggregation at 21:29:51 03-21-2019\n"
     ]
    }
   ],
   "source": [
    "# A dictionary, indexed by tablename+month, holds the resulting DataFrames\n",
    "aggregates = {}\n",
    "\n",
    "tables     = call_args.keys()\n",
    "tablewidth = (len(max(tables  , key=len)) + 1 +  # Add 1 for the '_'\n",
    "              len(max(suffixes, key=len)))       # Positions to reserve for table names (in output)\n",
    "rowswidth  = len(str(n_ids))                     # Positions to reserve for numbers of rows\n",
    "\n",
    "print('Started aggregation at {}'.format(datetime.now(tz).strftime('%H:%M:%S %m-%d-%Y')))\n",
    "for table, suffix in itertools.product(tables, suffixes):\n",
    "    start   = time()\n",
    "    key     = '{tab}_{suf}'.format(tab=table, suf=suffix)\n",
    "    print('Processing table {tab:>{wid}} ...'.format(tab=key, wid=tablewidth), end=' ')\n",
    "    aggregates[key] = agg_by_file(df_subset_ids.select(id_column), table, suffix, file_type, **call_args[table]).cache()\n",
    "\n",
    "    rows    = aggregates[key].count()\n",
    "    seconds = int(time() - start)\n",
    "    print('{r:>{w}d} rows processed in {t}.'.format(r=rows, w=rowswidth, t=timedelta(seconds=seconds)))\n",
    "\n",
    "print('Finished aggregation at {}'.format(datetime.now(tz).strftime('%H:%M:%S %m-%d-%Y')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Merge\"></a>\n",
    "## Merge all tables\n",
    "In the end, every ID gets a single row, with all columns from all variable tables (which are joined) for all months and one set of columns from all static tables (which are unioned).\n",
    "\n",
    "[Top **⤒**](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 510 rows from table customers_Jan.\n",
      "Adding 531 rows from table customers_Feb.\n",
      "Adding 551 rows from table customers_Mar.\n",
      "Adding 546 rows from table customers_Apr.\n",
      "Adding 559 rows from table customers_May.\n",
      "Adding 561 rows from table customers_Jun.\n",
      "Adding 545 rows from table customers_Jul.\n",
      "Adding 530 rows from table customers_Aug.\n",
      "Adding 526 rows from table customers_Sep.\n",
      "Adding 512 rows from table customers_Oct.\n",
      "Adding 493 rows from table customers_Nov.\n",
      "Adding 468 rows from table customers_Dec.\n",
      "Aggregating ... after aggregation, table customers_Jan contains 694 rows. Processed in 0:00:39\n"
     ]
    }
   ],
   "source": [
    "# Unions first, if any: they only work on tables of the same schema, so as long as no columns have been added\n",
    "df_empty  = spark.createDataFrame([], schema=StructType([StructField(id_column, StringType(), True)]))\n",
    "merge_msg = 'Adding {rows:>{rwid}} rows from table {tab:>{twid}}.'\n",
    "for table in [t for t in tables if merge_method[t] == 'union']:\n",
    "    # Initialize the union with the first month's table. Then iterate over the rest to add them.\n",
    "    start      = time()\n",
    "    suffix0    = suffixes[0]\n",
    "    key0       = '{tab}_{suf}'.format(tab=table, suf=suffix0)\n",
    "    df_current = aggregates[key0]\n",
    "    rows       = df_current.count()\n",
    "    print(merge_msg.format(rows=rows, rwid=rowswidth, tab=key0, twid=tablewidth))\n",
    "    \n",
    "    for suffix in suffixes[1:]:\n",
    "        key             = '{tab}_{suf}'.format(tab=table, suf=suffix)\n",
    "        rows            = aggregates[key].count()\n",
    "        print(merge_msg.format(rows=rows, rwid=rowswidth, tab=key, twid=tablewidth))\n",
    "        df_current      = df_current.union(aggregates[key])\n",
    "   \n",
    "    print('Aggregating ...', end=' ')\n",
    "    \n",
    "    # The same SQL query as was used for each month performs the aggregation over all months\n",
    "    df_current.orderBy('period').createOrReplaceTempView('extract')   # Must sort, otherwise First() and Last() don't make sense\n",
    "    query = call_args[table]['agg_query'].format(suffix0)\n",
    "    \n",
    "    # Replace the first-month's aggregate with the new overall aggregate; the rest was already emptied out\n",
    "    aggregates[key0] = spark.sql(query).cache()\n",
    "    rows             = aggregates[key0].count()\n",
    "    elapsed          = timedelta(seconds=int(time() - start))\n",
    "    print('after aggregation, table {tab} contains {rows} rows. Processed in {td}'.format(tab=key0, rows=rows, td=elapsed))\n",
    "    \n",
    "    # Reset the old monthly aggregate so it doesn't get rolled up again in a join\n",
    "    for suffix in suffixes[1:]:\n",
    "        aggregates['{tab}_{suf}'.format(tab=table, suf=suffix)] = None\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 2 columns: \"customer_id\" and \"churned\".\n",
      "Joining table     calls_Jan ...  7 columns added.\n",
      "Joining table customers_Jan ... 16 columns added.\n",
      "Joining table     calls_Feb ...  6 columns added.\n",
      "Joining table     calls_Mar ...  6 columns added.\n",
      "Joining table     calls_Apr ...  6 columns added.\n",
      "Joining table     calls_May ...  6 columns added.\n",
      "Joining table     calls_Jun ...  6 columns added.\n",
      "Joining table     calls_Jul ...  6 columns added.\n",
      "Joining table     calls_Aug ...  6 columns added.\n",
      "Joining table     calls_Sep ...  6 columns added.\n",
      "Joining table     calls_Oct ...  6 columns added.\n",
      "Joining table     calls_Nov ...  6 columns added.\n",
      "Joining table     calls_Dec ...  6 columns added.\n",
      "Processing ... Finished with 13 tables in 0:00:08. Final result has 91 columns and 694 rows\n"
     ]
    }
   ],
   "source": [
    "df_all   = df_subset_ids         # This time, include the churn column\n",
    "lastcols = 2\n",
    "n_tables = 0\n",
    "start    = time()\n",
    "colwidth = len(str(len(max([df.columns for df in aggregates.values() if df is not None], key=len))))\n",
    "\n",
    "print('Starting with 2 columns: \"{id}\" and \"{lbl}\".'.format(id=id_column, lbl=label_column))\n",
    "\n",
    "for suffix, table in itertools.product(suffixes, tables):\n",
    "    key        = '{tab}_{suf}'.format(tab=table, suf=suffix)\n",
    "    \n",
    "    if aggregates[key] is None: continue\n",
    "    print('Joining table {tab:>{wid}} ...'.format(tab=key, wid=tablewidth), end=' ')\n",
    "    \n",
    "    df_current = aggregates[key]\n",
    "    df_all     = df_all.join(df_current, on=[id_column], how='leftouter')\n",
    "#     df_current.unpersist()               # We don't need it anymore (NOTE: It's not clear if this helps)\n",
    "    \n",
    "    cols       = len(df_all.columns)\n",
    "    print('{col:>{cwid}d} columns added.'.format(col=cols-lastcols, cwid=colwidth))\n",
    "    lastcols   = cols\n",
    "    n_tables  += 1\n",
    "\n",
    "print('Processing ...', end=' ')\n",
    "df_all.cache()\n",
    "rows    = df_all.count()\n",
    "elapsed = timedelta(seconds=int(time() - start))\n",
    "print('Finished with {n} tables in {td}. Final result has {c} columns and {r} rows'.format(n=n_tables, td=elapsed, c=cols, r=rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Constants\"></a>\n",
    "## Aggregate remaining constant/current columns into one\n",
    "These are columns (attributes) that only need one value for the entire period, not monthly values.\n",
    "For aggregation over the whole period use the same method as was used for monthly aggregation\n",
    "\n",
    "**NOTE**: If the tables nicely separate the  constant and variable attributes, with each table\n",
    "having either all constant or all variable attributes, there is nothing more to do, because all\n",
    "this is already accomplished by unioning instead of joining the constant tables. This is the\n",
    "case in the sample dataset. The next cell is only needed if one or more tables have a mix of\n",
    "constant and variable attributes.\n",
    "\n",
    "[Top **⤒**](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Set up UDFs for use in column expressions, because, unlike sum and max, first and last\n",
    "# are not built-in aggregate functions.\n",
    "# PySpark supports ArrayType columns; ArrayType values behave like (or really are) lists;\n",
    "# therefore, standard Python methods next() (notice the default parameter) and reversed() work.\n",
    "# Use the pyspark.sql function array() to create the lists passed as arguments into these UDFs.\n",
    "# There may be other ways to achieve the same effect, but this seems reasonably straightforward.\n",
    "first_udf = udf(lambda array: next((v for v in          array  if v is not None), None), StringType())\n",
    "last_udf  = udf(lambda array: next((v for v in reversed(array) if v is not None), None), StringType())\n",
    "\n",
    "# Associate the right aggregate with each attribute\n",
    "# (\"Attribute\" instead of \"column\", because each attribute is associated with multiple columns,\n",
    "# one for each month. The purpose of the following operation is to end up with one column for each.)\n",
    "attribute_aggregate = {}    # {attr1:agg1, attr2:agg2, ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Use a simple for loop\n",
    "Don't worry about code efficiency: because Spark uses lazy evaluation, no action is\n",
    "performed until the result needs to be instantiated. That allows Spark to accumulate\n",
    "transformations and optimize the entire sequence before it executes anything.\n",
    "Therefore, it doesn't matter if you use loops, list comprehensions, massive SQL queries,\n",
    "or any other clever code to achieve the result; the execution plan will look the same.\n",
    "\n",
    "(Earlier cells in this notebook also rely on lazy evaluation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for attribute, aggregate in attribute_aggregate.items():                   # March through the columns to be aggregated\n",
    "    columns = [c for c in df_all.columns if c.startswith(attribute + '_')] # <attr-name>_Jan, <attr-name>_Feb, etc.\n",
    "    df_all = (df_all\n",
    "              .withColumn(attribute, aggregate(array(columns)))            # Add the single, whole-period column\n",
    "              .drop(*columns)                                              # Drop the monthly columns\n",
    "             )\n",
    "    \n",
    "df_all = df_all.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_id',\n",
       " 'churned',\n",
       " 'period_count',\n",
       " 'from_calls_Jan',\n",
       " 'from_duration_Jan',\n",
       " 'from_dropped_Jan',\n",
       " 'to_calls_Jan',\n",
       " 'to_duration_Jan',\n",
       " 'to_dropped_Jan',\n",
       " 'gender',\n",
       " 'age',\n",
       " 'type',\n",
       " 'location',\n",
       " 'callcenter_callcount',\n",
       " 'text_package_deal',\n",
       " 'voice_package_deal',\n",
       " '4g_handset_deal',\n",
       " 'all_u_can_eat_data_deal',\n",
       " 'accidental_damage_cover_deal',\n",
       " 'europe_roamer_deal',\n",
       " 'americas_roamer_deal',\n",
       " 'asia_africa_roamer_deal',\n",
       " 'extra_handset_deal',\n",
       " 'musicsubscription_deal',\n",
       " 'period',\n",
       " 'from_calls_Feb',\n",
       " 'from_duration_Feb',\n",
       " 'from_dropped_Feb',\n",
       " 'to_calls_Feb',\n",
       " 'to_duration_Feb',\n",
       " 'to_dropped_Feb',\n",
       " 'from_calls_Mar',\n",
       " 'from_duration_Mar',\n",
       " 'from_dropped_Mar',\n",
       " 'to_calls_Mar',\n",
       " 'to_duration_Mar',\n",
       " 'to_dropped_Mar',\n",
       " 'from_calls_Apr',\n",
       " 'from_duration_Apr',\n",
       " 'from_dropped_Apr',\n",
       " 'to_calls_Apr',\n",
       " 'to_duration_Apr',\n",
       " 'to_dropped_Apr',\n",
       " 'from_calls_May',\n",
       " 'from_duration_May',\n",
       " 'from_dropped_May',\n",
       " 'to_calls_May',\n",
       " 'to_duration_May',\n",
       " 'to_dropped_May',\n",
       " 'from_calls_Jun',\n",
       " 'from_duration_Jun',\n",
       " 'from_dropped_Jun',\n",
       " 'to_calls_Jun',\n",
       " 'to_duration_Jun',\n",
       " 'to_dropped_Jun',\n",
       " 'from_calls_Jul',\n",
       " 'from_duration_Jul',\n",
       " 'from_dropped_Jul',\n",
       " 'to_calls_Jul',\n",
       " 'to_duration_Jul',\n",
       " 'to_dropped_Jul',\n",
       " 'from_calls_Aug',\n",
       " 'from_duration_Aug',\n",
       " 'from_dropped_Aug',\n",
       " 'to_calls_Aug',\n",
       " 'to_duration_Aug',\n",
       " 'to_dropped_Aug',\n",
       " 'from_calls_Sep',\n",
       " 'from_duration_Sep',\n",
       " 'from_dropped_Sep',\n",
       " 'to_calls_Sep',\n",
       " 'to_duration_Sep',\n",
       " 'to_dropped_Sep',\n",
       " 'from_calls_Oct',\n",
       " 'from_duration_Oct',\n",
       " 'from_dropped_Oct',\n",
       " 'to_calls_Oct',\n",
       " 'to_duration_Oct',\n",
       " 'to_dropped_Oct',\n",
       " 'from_calls_Nov',\n",
       " 'from_duration_Nov',\n",
       " 'from_dropped_Nov',\n",
       " 'to_calls_Nov',\n",
       " 'to_duration_Nov',\n",
       " 'to_dropped_Nov',\n",
       " 'from_calls_Dec',\n",
       " 'from_duration_Dec',\n",
       " 'from_dropped_Dec',\n",
       " 'to_calls_Dec',\n",
       " 'to_duration_Dec',\n",
       " 'to_dropped_Dec']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity test: check if all expected columns are there\n",
    "[f.name for f in df_all.schema.fields]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id=\"Output\"></a>\n",
    "## Write the results to file\n",
    "Write a Parquet file for ease of loading into Spark, and a compressed CSV file (with header) for any other purpose.\n",
    "\n",
    "[Top **⤒**](#Top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 7\n"
     ]
    }
   ],
   "source": [
    "print('Number of partitions: {}'.format(df_all.rdd.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user-home/1053/DSX_Projects/Big-Churn/datasets/subset_merge_agg_0k_35.parquet written.\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(dir_path, out_file + '.parquet')\n",
    "df_all.write.parquet(filename, mode='overwrite')\n",
    "print('{} written.'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Write a single CSV file from a Spark DataFrame\n",
    "This relies on a custom function. Its module must be accessible through the python path (`sys.path`), for example by having it installed in the project's `scripts` directory and having added that directory to the python path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-22 04:30:43,057 - INFO - File already exists.\n",
      "2019-03-22 04:30:43,058 - INFO - Existing file will be overwritten.\n",
      "2019-03-22 04:30:44,100 - INFO - The file /user-home/1053/DSX_Projects/Big-Churn/datasets/subset_merge_agg_0k_35.csv.gz (0.1 MB) is now available as a single CSV file.\n"
     ]
    }
   ],
   "source": [
    "# Set up logging to capture output from the function instead of letting it operate silently.\n",
    "logger    = logging.getLogger('spark1csv')\n",
    "handler   = logging.StreamHandler()\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "handler.setLevel(logging.INFO)   # Can set these to logging.DEBUG to get more verbose output\n",
    "logger .setLevel(logging.INFO)\n",
    "logger .addHandler(handler)\n",
    "\n",
    "filename = os.path.join(dir_path, out_file + '.csv.gz')\n",
    "spark_write_one_csv(df_all, filename) # Use default settings: gzip compression, add header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developed by IBM Data Science Elite Team, IBM Data Science and AI:\n",
    "- Robert Uleman, Data Science Engineer\n",
    "\n",
    "Copyright (c) 2019 IBM Corporation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.5 with Watson Studio Spark 2.2.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
